"""
RAGAS Testing Module
====================
This module provides functionality for evaluating RAG (Retrieval-Augmented Generation)
systems using the RAGAS framework.

RAGAS metrics include:
- Faithfulness: Measures factual consistency of the answer with the context
- Answer Relevancy: Measures how relevant the answer is to the question
- Context Precision: Measures signal-to-noise ratio in retrieved contexts
- Context Recall: Measures if all relevant information was retrieved
"""

import os
from typing import List, Dict, Any
from datasets import Dataset
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_precision,
    context_recall,
)


def create_ragas_dataset(
    questions: List[str],
    answers: List[str],
    contexts: List[List[str]],
    ground_truths: List[str] = None
) -> Dataset:
    """
    Create a dataset for RAGAS evaluation.
    
    Args:
        questions: List of questions asked
        answers: List of generated answers
        contexts: List of retrieved context chunks (each is a list of strings)
        ground_truths: Optional list of reference answers
        
    Returns:
        Dataset object compatible with RAGAS
    """
    data = {
        "question": questions,
        "answer": answers,
        "contexts": contexts,
    }
    
    if ground_truths:
        data["ground_truth"] = ground_truths
    
    return Dataset.from_dict(data)


def evaluate_rag_system(
    questions: List[str],
    answers: List[str],
    contexts: List[List[str]],
    ground_truths: List[str] = None,
    metrics: List[Any] = None
) -> Dict[str, float]:
    """
    Evaluate a RAG system using RAGAS metrics.
    
    Args:
        questions: List of questions
        answers: List of generated answers
        contexts: List of retrieved contexts
        ground_truths: Optional reference answers
        metrics: List of RAGAS metrics to use (default: all available)
        
    Returns:
        Dictionary containing evaluation scores
    """
    # Default metrics
    if metrics is None:
        metrics = [faithfulness, answer_relevancy]
        if ground_truths:
            metrics.extend([context_precision, context_recall])
    
    # Create dataset
    dataset = create_ragas_dataset(questions, answers, contexts, ground_truths)
    
    # Run evaluation
    results = evaluate(dataset, metrics=metrics)
    
    return results


def run_example_evaluation():
    """
    Run an example RAGAS evaluation with needle-in-a-haystack queries.
    """
    # Needle-in-a-haystack test queries
    questions = [
        "What is the claim date?",      
        "What is the claim amount?",
        "When was the incident reported?",
        "When did the incident occur?",  
        "How many days elapsed between the incident date and when the claim was filed?"
        "What is the exact purchase order number referenced for the February 5–12, 2024 maintenance work that allegedly addressed the water intrusion?",
        "According to the chronological event timeline, at what precise time did the morning shift supervisor discover water pooling in the basement mechanical room and second‑floor production area on March 12, 2024?",
        "In the March 14, 2024 loss assessor's preliminary assessment, what percentage of the facility is estimated to be affected by damage, and to which specific areas of the building is that damage confined?",
        
        "How would you comprehensively summarize the insurance claim, including the parties involved, nature and cause of loss, claimed amounts, and current investigation status?",
        "What is the complete chronological timeline of all major events from the initial February seepage observations through the anticipated resolution dates, including inspections, reports, and claim-handling milestones?",
        "What are the main coverage, documentation, and causation issues identified in the claim, and how might they affect the insurer’s final decision on property damage and business interruption payments?"
    ]
    
    # You'll need to provide answers generated by your RAG system
    answers = [
        # TODO: Replace with actual answers from your RAG system
        "The claim was filed on March 15, 2024 at 11:00 AM.",
        "The claim amount is $387,500.00.",   
        "The incident was reported on March 15, 2024.",
        "The incident occurred on March 12, 2024 at 2:47 AM.",
        "The exact duration between the incident date and when the claim was filed is 3 days, 8 hours, and 13 minutes, totaling 80.22 hours.",
        "The loss location is 2847 Industrial Drive, Newark, NJ 07105.",
        "The morning shift supervisor discovered water pooling in the basement mechanical room and second-floor production area on March 12, 2024, at 6:30 AM.",
        "The loss assessor's preliminary assessment on March 14, 2024, did not provide specific information on the percentage of the facility estimated to be affected by damage or the specific areas of the building where the damage is confined."
    ]
    
    # You'll need to provide the contexts retrieved by your RAG system
    contexts = [
        # TODO: Replace with actual contexts retrieved from your vector store
        ["Claim filed on March 15, 2024 for water damage incident."],
        ["Policyholder: ABC Manufacturing Corporation, Policy #: COM-2024-789456"],
        ["Total estimated loss: $2,500,000 USD"],
        ["Incident first reported to insurance on March 12, 2024 at 7:30 AM"],
        ["Previous maintenance work (PO-2024-0205-WI) conducted February 5-12, 2024"],
        ["6:45 AM - Morning supervisor discovers water pooling in basement mechanical room and second-floor production area"],
        ["Preliminary assessment shows 35% facility damage, primarily basement and second floor"],
    ]
    
    # Optional: Ground truth answers if available
    ground_truths = [
        "March 15, 2024",
        "ABC Manufacturing Corporation",
        "$2,500,000",
        "March 12, 2024",
        "PO-2024-0205-WI",
        "6:45 AM on March 12, 2024",
        "35% of the facility, confined to basement mechanical room and second-floor production area",
    ]
    
    print("Running RAGAS evaluation with needle-in-a-haystack queries...")
    results = evaluate_rag_system(questions, answers, contexts, ground_truths)
    
    print("\n=== RAGAS Evaluation Results ===")
    print(results)
    
    return results


def evaluate_custom_rag_output(
    questions: List[str],
    rag_answers: List[str],
    rag_contexts: List[List[str]],
    ground_truths: List[str] = None
) -> Dict[str, float]:
    """
    Evaluate custom RAG system output with the needle-in-a-haystack queries.
    
    Args:
        questions: List of needle-in-a-haystack questions
        rag_answers: Answers generated by your RAG system
        rag_contexts: Contexts retrieved by your RAG system
        ground_truths: Optional ground truth answers
        
    Returns:
        Dictionary containing evaluation scores
    """
    return evaluate_rag_system(questions, rag_answers, rag_contexts, ground_truths)


if __name__ == "__main__":
    # Run example evaluation
    try:
        results = run_example_evaluation()
    except ImportError as e:
        print(f"Error: {e}")
        print("\nTo use RAGAS, install required packages:")
        print("pip install ragas datasets")
    except Exception as e:
        print(f"Error during evaluation: {e}")
        import traceback
        traceback.print_exc()
